{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL\n",
    "# Architecture\n",
    "def inference(x):\n",
    "    phase_train = tf.constant(True)\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    conv1 = tf.layers.conv2d(inputs=x, filters=32,  kernel_size=[7, 7], padding=\"same\", activation=tf.nn.relu)\n",
    "    norm1 = tf.layers.batch_normalization(conv1)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=norm1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    conv2a = tf.layers.conv2d(inputs=pool1, filters=32, kernel_size=[1, 1], padding=\"same\", activation=tf.nn.relu)\n",
    "    conv2 = tf.layers.conv2d(inputs=conv2a, filters=64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "    norm2 = tf.layers.batch_normalization(conv2)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=norm2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    conv3a = tf.layers.conv2d(inputs=pool2, filters=32, kernel_size=[1, 1], padding=\"same\", activation=tf.nn.relu)\n",
    "    conv3 = tf.layers.conv2d(inputs=conv3a, filters=64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "    conv4a = tf.layers.conv2d(inputs=conv3, filters=32, kernel_size=[1, 1], padding=\"same\", activation=tf.nn.relu)\n",
    "    conv4 = tf.layers.conv2d(inputs=conv4a, filters=64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu)\n",
    "\n",
    "    flat = tf.reshape(conv4, [-1, 7 * 7 * 64])\n",
    "\n",
    "    fc_1 = tf.layers.dense(inputs=flat, units=128, activation=tf.nn.relu)\n",
    "\n",
    "    embed = tf.layers.dense(inputs=fc_1, units=128)\n",
    "\n",
    "    output = tf.nn.l2_normalize(embed, dim=1, epsilon=1e-12, name=None)\n",
    "\n",
    "    return output\n",
    "\n",
    "def loss(output, labels):\n",
    "    triplet = tf.contrib.losses.metric_learning.triplet_semihard_loss(labels, output, margin=margin)\n",
    "    loss = tf.reduce_mean(triplet, name='triplet')\n",
    "    return loss\n",
    "\n",
    "def training(cost, global_step,learning_rate,decay_steps, decay_rate):\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    learning_rate = tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op\n",
    "\n",
    "def load_data():\n",
    "    # the data, split between train and test sets\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "        x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    print('y_train shape:', y_train.shape)\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Parameters\n",
    "    training_epochs = 100\n",
    "    display_step = 1\n",
    "    batch_size = 128\n",
    "    margin = 1.0\n",
    "    learning_rate = 1e-2\n",
    "    decay_steps = 1e2\n",
    "    decay_rate = 0.9\n",
    "    # input image dimensions\n",
    "    img_rows, img_cols = 28, 28\n",
    "\n",
    "    # the data, split between train and test sets\n",
    "    x_train, y_train, x_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x = tf.placeholder(\"float\", [None, 28, 28, 1], name='placehold_x')\n",
    "    y = tf.placeholder('float', [None], name='placehold_y')\n",
    "\n",
    "    output = inference(x)\n",
    "    tf.identity(output, name=\"inference\")\n",
    "\n",
    "    cost = loss(output, y)\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    train_op = training(cost, global_step, learning_rate,decay_steps, decay_rate)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    \"\"\"Making iterator\"\"\"\n",
    "    features_placeholder = tf.placeholder(x_train.dtype, x_train.shape)\n",
    "    labels_placeholder = tf.placeholder(y_train.dtype, y_train.shape)\n",
    "\n",
    "    training_dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
    "    batched_dataset = training_dataset.batch(batch_size)\n",
    "\n",
    "    training_init_op = batched_dataset.make_initializable_iterator()\n",
    "    next_element = training_init_op.get_next()\n",
    "\n",
    "    \"\"\"Training\"\"\"\n",
    "    sess = tf.Session()\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        with sess.as_default():\n",
    "\n",
    "            # Training cycle\n",
    "            for epoch in range(training_epochs):\n",
    "                avg_cost = 0.\n",
    "                total_batch_train = int(x_train.shape[0] / batch_size)\n",
    "                sess.run(training_init_op.initializer, feed_dict={features_placeholder: x_train,\n",
    "                                                                  labels_placeholder: y_train})\n",
    "                # Loop over all batches\n",
    "                for i in range(total_batch_train):\n",
    "                    # Fit training using batch data\n",
    "                    batch_x, batch_y = sess.run(next_element)\n",
    "\n",
    "                    sess.run(train_op, feed_dict={x: batch_x, y: batch_y})\n",
    "                    # Compute average loss\n",
    "                    avg_cost = sess.run(cost, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "                    if not i % 10:\n",
    "                        print('Epoch #: ', epoch, 'global step', sess.run(global_step), '  Batch #: ', i, 'loss: ',\n",
    "                              avg_cost)\n",
    "\n",
    "#                 saver.save(sess, \"model_logs/model-checkpoint\", global_step=global_step, write_meta_graph=True)\n",
    "\n",
    "            print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
